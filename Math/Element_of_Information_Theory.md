# 信息论基础

## 1 绪论与概览

## 2 熵 相对熵与互信息

### 2.1 熵

$$ H(X) = - \sum_{x \in X}{p(x)\log{p(x)}} $$

### 2.2 联合熵

$$ H(X, Y) = - \sum_{x \in X} \sum_{y \in Y} p(x,y)\log{p(x,y)}$$

**定理2.2.1(链式法则)**: $$H(X, Y) = H(x) + H(Y | X)$$

### 2.3 相对熵与互信息

**相对熵(relative entropy)**:
  $$D(p || q) = \sum_{x \ in X} p(x) \log{\frac{p(x)}{q(x)}}=E_p \log{\frac{p(x)}{q(x)}}$$

**互信息(mutual information)**:
  $$I(X;Y) = \sum_{x \in X} \sum_{y \in Y} p(x,y) \log{\frac{p(x,y)}{p(x)p(y)}} = D(p(x,y)||p(x)p(y))$$

### 2.4 熵与互信息的关系

$$I(X;Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)$$

互信息I(X;Y)是在给定Y知识的条件下X的不确定度的缩减量

$$I(X;Y) = H(X) + H(Y) - H(X, Y)$$

### 2.5 熵,相对熵与互信息的链式法则

**定理2.5.1(熵的链式法则)**: $$H(X_1, X_2, \dots, X_n) = \sum_{i = 1}^n H(X_i|X_{i-1}, \dots, X_1)$$

**定理2.5.2(互信息的链式法则)**: $$I(X_1, X_2, \dots, X_n;Y) = \sum_{i = 1}^n I(X_i;Y|X_{i-1}, \dots, X_1)$$

**条件相对熵**: $$D(p(y|x)||q(y|x)) = \sum_x p(x) \sum_y p(y|x) \log{\frac{p(y|x)}{q(y|x)}} = E_{p(x,y)}\log{\frac{p(Y|X)}{q(Y|X)}}$$

**定理2.5.3(相对熵的链式法则)**: $$D(p(x,y) || q(x,y)) = D(p(x)||q(x)) + D(p(y|x)||q(y|x))$$

### 2.6 Jensen不等式及其结果

**定理2.6.2(Jensen不等式)**: 若给定凸函数f和一个随机变量X,则 $$Ef(X) \ge f(EX)$$

**定理2.6.3(信息不等式)**: $$D(p||q) \ge 0$$

**推论(互信息的非负性)**: $$I(X;Y) \ge 0$$

**定理2.6.4**: $$H(X) \le \log{|X|}$$

**定理2.6.5(条件作用使熵减小)**: $$H(X|Y) \le H(X)$$

从直观上讲,此定理说明知道另一随机变量Y的信息只会降低X的不确定度. 注意这仅对平均意义成立. 具体来说, H(X | Y = y)可能比H(X)大或者小,或者两者相等.

**定理2.6.6(熵的独立界)**: $$H(X_1, X_2, \dots, X_n) \le \sum_{i=1}^n H(X_i)$$

### 2.7 对数和不等式及其应用

**定理2.7.1(对数和不等式)**: $$\sum_{i = 1}^n a_i \log{\frac{a_i}{b_i}} \ge (\sum_{i=1}^n a_i)\log{\frac{\sum_{i = 1}^n a_i}{\sum_{i = 1}^n b_i}}$$

**定理2.7.2(相对熵的凸性)**: D(p || q) 关于对(p,q)是凸的

**定理2.7.3(熵的凹性)**: H(p)是关于p的凹函数

### 2.8 数据处理不等式

### 2.9 充分统计量

这节很有意思,利用统计量代替原有抽样,并且不损失信息.

### 2.10 费诺不等式

**定理2.10.1(费诺不等式)**: 对任何满足 $$X \to Y \to\hat{X},$$ 设 $$P_e = Pr\{X \neq \hat{X}\},$$ 有

  $$H(P_e) + P_e \log{|\mathcal{X}|} \ge H(X | \hat{X}) \ge H(X | Y)$$

上述不等式可以减弱为

  $$1 + P_e \log{|\mathcal{X}|} \ge H(X | Y)$$ 或 $$P_e \ge \frac{H(X|Y) - 1}{\log{|\mathcal{X}|}}$$

**引理 2.10.1**: 如果X和X'独立同分布,具有熵H(X),则

  $$Pr(X = X') \ge 2^{-H(X)}$$
  
## 3 渐进均分性








