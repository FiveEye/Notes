#A Background Material

##A.1 Information Theory

###A.1.1 Compression and Entropy

**Definition A.1**: Let P(X) be a distribution over a random variable X. The entropty of X is defined as

  $$H_P(X) = E_P[\log{\frac{1}{P(x)}}] = \sum_x P(x) \log{\frac{1}{P(x)}},$$

where we treat

  $$0\log{1/0} = 0.$$


##A.2 Convergence Bounds

##A.3 Algorithms and Algorithmic Complexity

##A.4 Combinatorial Optimization and Search

##A.5 Continuous Optimization
